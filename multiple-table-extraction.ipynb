{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::393747608406:role/gluelearning-access-jogesh\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 0412df71-24fc-4a27-b02e-44a9953c36b6\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session 0412df71-24fc-4a27-b02e-44a9953c36b6 to get into ready status...\nSession 0412df71-24fc-4a27-b02e-44a9953c36b6 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Sample code To perform Multiple table Extraction and merge them simultaneously",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Defining the source information in a dictionary\n\"\"\"\nExample:\ndict = {\n\"database_name\": {\n\"table_name\": []\n}\n}\n\"\"\"\nconfig = {\n\"pyspark_123_db\":{\n\"customers\": [],\n\"employees\": [],\n\"orders\":[]\n}\n}\n\n# Loop to create Glue Dynamicframes.\ndict_dyf = {}\nfor db_name in config:\n    source_database_name = db_name\n    for tbl_name in config[db_name]:\n        source_table_name = tbl_name\n        a = glueContext.create_dynamic_frame.from_catalog(\n        database = source_database_name,\n        table_name = source_table_name,\n        transformation_ctx = \"extract_data\"\n        )\n        dict_dyf.update({tbl_name:a})\n            \n        ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 62,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Checking the Schema for the data and correcting the same",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "##dict_dyf['customers'].printSchema()\nmappings = [(\"customer_id\",\"choice\",\"customer_id\",\"bigint\"), (\"first_name\", \"string\",\"first_name\", \"string\"),(\"last_name\", \"string\",\"last_name\", \"string\"), (\"full_name\", \"string\",\"full_name\", \"string\")]\ndyf_output = ApplyMapping.apply(\nframe = dict_dyf['customers'],\nmappings = mappings,\ntransformation_ctx = \"ctx\"\n)\n\n##dyf_output.printSchema()\n\nmappings = [(\"customer_id\",\"long\",\"customer_id\",\"bigint\"), (\"first_name\", \"string\",\"first_name\", \"string\"),(\"last_name\", \"string\",\"last_name\", \"string\"), (\"full_name\", \"string\",\"full_name\", \"string\")]\ndyf = ApplyMapping.apply(\nframe = dyf_output,\nmappings = mappings,\ntransformation_ctx = \"ctx\"\n)\n\n##dyf.printSchema()\n\ndict_dyf['customers'] = dyf\ndict_dyf['customers'].printSchema()\ndict_dyf['employees'].printSchema()\ndict_dyf['orders'].printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 58,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- customer_id: long\n|-- first_name: string\n|-- last_name: string\n|-- full_name: string\n\nroot\n|-- employee_id: long\n|-- manager_id: choice\n|    |-- long\n|    |-- string\n|-- first_name: string\n|-- \tlast_name: string\n|-- \tfull_name: string\n|-- \tjobtitle: string\n|-- \torganizationlevel: long\n|-- maritalstatus: string\n|-- gender\t: string\n|-- territory: string\n|-- country: string\n|-- group: string\n\nroot\n|-- salesorder_id: long\n|-- \tsalesorderdetail_id: long\n|-- \torderdate: string\n|-- duedate: string\n|-- shipdate: string\n|-- employee_id: long\n|-- customer_id: long\n|-- \tsubtotal: double\n|-- taxamt: double\n|-- freight: double\n|-- totaldue: double\n|-- product_id: long\n|-- \torderqty: long\n|-- unitprice: double\n|-- unitpricediscount: double\n|-- \tlinetotal: double\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Viewing the stored Dynamic frame in Dictionary",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "print(dict_dyf)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'customers': <awsglue.dynamicframe.DynamicFrame object at 0x7f420b7d2f50>, 'employees': <awsglue.dynamicframe.DynamicFrame object at 0x7f420b732b50>, 'orders': <awsglue.dynamicframe.DynamicFrame object at 0x7f420b7325d0>}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## CREATING VIEWS",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#Defining Master table Dictionary\n\"\"\"\ndict = {\n\"table\": \"master_table_name\"\n}\n\"\"\"\nmaster_table = {\n\"table\": \"orders\"\n}\n\n# Loop to create views\nfor i,j in dict_dyf.items():\n    for k in master_table.values():\n        if i == k:\n            master_table = j\n            master_df = master_table.toDF()\n            master_df.createOrReplaceTempView(\"master\")\n        else:\n            dim_table = j\n            dim_df = dim_table.toDF()\n            dim_df.createOrReplaceTempView(f\"{i}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 60,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Performing the Joins to the master table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#Defining the dictionary containing the dimension table Unique key Mapping to Master Dataset\n\"\"\"\nExample: \ndict = {\n\"table_name\": {\"dim_table_unique_key\": \"master_table_unique_key\"}\n}\n\"\"\"\n\ndim_table_mapping = {\n\"customers\": {\"customer_id\": \"customer_id\"},\n\"employees\": {\"employee_id\": \"employee_id\"}\n}\n\nfor i,j in dim_table_mapping.items():\n    for k,l in j.items():\n        dim_psdf = spark.sql(f\"SELECT * FROM {i}\")\n        dim_psdf = dim_psdf.withColumnRenamed(f\"{k}\", \"extra\")\n        dim_psdf.createOrReplaceTempView(f\"{i}\")\n        spark.sql(f\"\"\"\n                SELECT * FROM master\n                INNER JOIN {i}\n                ON master.{l} = {i}.extra\n                \"\"\").createOrReplaceTempView(\"data\")\n        psdf_out = spark.sql(\"SELECT * FROM data\")\n        psdf_out = psdf_out.drop(\"extra\")\n        psdf_out.createOrReplaceTempView(\"data\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 61,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Viewing the Schema for Final Merged Data",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Converting the view to Pyspark Dataframe\npsdf = spark.sql(\"SELECT * FROM data\")\npsdf.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 63,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- salesorder_id: long (nullable = true)\n |-- \tsalesorderdetail_id: long (nullable = true)\n |-- \torderdate: string (nullable = true)\n |-- duedate: string (nullable = true)\n |-- shipdate: string (nullable = true)\n |-- employee_id: long (nullable = true)\n |-- customer_id: long (nullable = true)\n |-- \tsubtotal: double (nullable = true)\n |-- taxamt: double (nullable = true)\n |-- freight: double (nullable = true)\n |-- totaldue: double (nullable = true)\n |-- product_id: long (nullable = true)\n |-- \torderqty: long (nullable = true)\n |-- unitprice: double (nullable = true)\n |-- unitpricediscount: double (nullable = true)\n |-- \tlinetotal: double (nullable = true)\n |-- manager_id: struct (nullable = true)\n |    |-- long: long (nullable = true)\n |    |-- string: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- \tlast_name: string (nullable = true)\n |-- \tfull_name: string (nullable = true)\n |-- \tjobtitle: string (nullable = true)\n |-- \torganizationlevel: long (nullable = true)\n |-- maritalstatus: string (nullable = true)\n |-- gender\t: string (nullable = true)\n |-- territory: string (nullable = true)\n |-- country: string (nullable = true)\n |-- group: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Writing the data to Glue Catalog and partitioning it to S3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import current_timestamp\npsdf = psdf.withColumn(\"load_date\", current_timestamp())\ngdyf = DynamicFrame.fromDF(psdf, glueContext, \"convert\")\ngdyf.printSchema()\nwrite_parquet = glueContext.getSink(\npath = \"s3://gluepractice-jogesh-bucket/temp-dir/\",\nconnection_type = \"s3\",\nupdateBehaviour = \"LOG\",\npartitionKeys = \"load_date\",\ncompression = \"snappy\",\nenableDataCatalog = True,\ntransformation_ctx = 'load_data'\n)\nwrite_parquet.setCatalogInfo(catalogDatabase = 'retail_opsreadiness_datalake', catalogTableName = 'processed_customer_data')\nwrite_parquet.setFormat(\"glueparquet\")\nwrite_parquet.writeFrame(gdyf)",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- salesorder_id: long\n|-- \tsalesorderdetail_id: long\n|-- \torderdate: string\n|-- duedate: string\n|-- shipdate: string\n|-- employee_id: long\n|-- customer_id: long\n|-- \tsubtotal: double\n|-- taxamt: double\n|-- freight: double\n|-- totaldue: double\n|-- product_id: long\n|-- \torderqty: long\n|-- unitprice: double\n|-- unitpricediscount: double\n|-- \tlinetotal: double\n|-- manager_id: struct\n|    |-- long: long\n|    |-- string: string\n|-- first_name: string\n|-- \tlast_name: string\n|-- \tfull_name: string\n|-- \tjobtitle: string\n|-- \torganizationlevel: long\n|-- maritalstatus: string\n|-- gender\t: string\n|-- territory: string\n|-- country: string\n|-- group: string\n|-- load_date: timestamp\n\n<awsglue.dynamicframe.DynamicFrame object at 0x7f420b7f5410>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}